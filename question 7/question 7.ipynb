{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2df81-7ba5-4263-bbec-51f1eeb3e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "label_mapping={\"with_mask\":1, \"without_mask\": 2, \"mask_weared_incorrect\": 3}\n",
    "\n",
    "class MaskedFaceDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = [os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png') or x.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Load XML annotation\n",
    "        annotation_path = img_path.replace('.png', '.xml').replace('.jpg', '.xml')\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for member in root.findall('object'):\n",
    "            bbox = member.find('bndbox')\n",
    "            boxes.append([float(bbox.find('xmin').text), float(bbox.find('ymin').text),\n",
    "                          float(bbox.find('xmax').text), float(bbox.find('ymax').text)])\n",
    "            label = member.find('name').text  # Assuming labels are strings like 'category1'\n",
    "            labels.append(label)  # This will need to be converted to integers based on your label mapping\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor([label_mapping[label] for label in labels], dtype=torch.int64)  # Convert string labels to integers\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def get_model(num_classes):\n",
    "    # Load a pre-trained model for object detection\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.DEFAULT')\n",
    "\n",
    "    # Replace the classifier with a new one for num_classes (3 classes + background)\n",
    "    num_classes = 4  # 3 classes + background\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def count_bounding_boxes(predictions, num_categories=3):\n",
    "    counts = []\n",
    "    for prediction in predictions:\n",
    "        image_counts = [0] * num_categories\n",
    "        labels = prediction['labels']\n",
    "        for i in range(1, num_categories + 1):  # Categories are assumed to be labeled as 1, 2, ..., num_categories\n",
    "            image_counts[i-1] = (labels == i).sum().item()\n",
    "        counts.append(image_counts)\n",
    "    return counts\n",
    "\n",
    "\n",
    "def parse_xml_for_counts(image_dir, num_categories=3, category_names=None):\n",
    "    if category_names is None:\n",
    "        category_names = [f'category{i}' for i in range(1, num_categories + 1)]\n",
    "\n",
    "    actual_counts = []\n",
    "    for xml_file in os.listdir(image_dir):\n",
    "        if xml_file.endswith('.xml'):\n",
    "            tree = ET.parse(os.path.join(image_dir, xml_file))\n",
    "            root = tree.getroot()\n",
    "\n",
    "            image_counts = [0] * num_categories\n",
    "            for member in root.findall('object'):\n",
    "                label = member.find('name').text\n",
    "                if label in category_names:\n",
    "                    index = category_names.index(label)\n",
    "                    image_counts[index] += 1\n",
    "\n",
    "            actual_counts.append(image_counts)\n",
    "\n",
    "    return actual_counts\n",
    "\n",
    "\n",
    "# Example usage in a validation loop\n",
    "def validate_model(model, val_loader, image_dir):\n",
    "    model.eval()\n",
    "    all_counts = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "\n",
    "            outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "            image_counts = count_bounding_boxes(outputs)\n",
    "            all_counts.extend(image_counts)\n",
    "\n",
    "    actual_counts = parse_xml_for_counts(image_dir)\n",
    "    return all_counts, actual_counts\n",
    "\n",
    "# Define transformations\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "path_to_train_dataset=\"/content/drive/MyDrive/MaskedFace/train\"\n",
    "path_to_val_dataset=\"/content/drive/MyDrive/MaskedFace/val\"\n",
    "# Load datasets and create data loaders\n",
    "train_dataset = MaskedFaceDataset(path_to_train_dataset, transform=transform)\n",
    "val_dataset = MaskedFaceDataset(path_to_val_dataset, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Create the model and move it to the appropriate device\n",
    "model = get_model(num_classes=4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training Logic\n",
    "num_epochs = 28\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, targets in tqdm(train_loader,desc='progress',leave=False):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {losses.item()}\")\n",
    "\n",
    "\n",
    "# Validation Logic\n",
    "predicted_counts, actual_counts = validate_model(model, val_loader, \"/content/drive/MyDrive/MaskedFace/val\")\n",
    "\n",
    "# Comparing Predicted Counts with Actual Counts\n",
    "def compare_counts(predicted, actual):\n",
    "    for i in range(len(predicted)):\n",
    "        print(f\"Image {i+1}:\")\n",
    "        print(f\"Predicted: {predicted[i]}, Actual: {actual[i]}\")\n",
    "\n",
    "compare_counts(predicted_counts, actual_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16858b74-9f0c-44d9-9df1-2e0429da07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(actual_counts, predicted_counts):\n",
    "    total_mape = 0\n",
    "    for ac, pc in zip(actual_counts, predicted_counts):\n",
    "        image_mape = 0\n",
    "        for a, p in zip(ac, pc):\n",
    "            image_mape += abs(a - p) / max(a, 1) * 100\n",
    "        total_mape += image_mape / len(ac)\n",
    "    return total_mape / len(actual_counts)\n",
    "\n",
    "def count_masks(model, val_loader, image_dir):\n",
    "    predicted_counts, actual_counts = validate_model(model, val_loader, image_dir)\n",
    "    mape_score = calculate_mape(actual_counts, predicted_counts)\n",
    "    predicted_counts_np = np.array(predicted_counts, dtype=np.int64)\n",
    "    return predicted_counts_np, mape_score\n",
    "\n",
    "# Example usage\n",
    "predicted_counts_np, mape_score = count_masks(model, val_loader, \"/content/drive/MyDrive/MaskedFace/val\")\n",
    "print(f\"MAPE Score: {mape_score}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
